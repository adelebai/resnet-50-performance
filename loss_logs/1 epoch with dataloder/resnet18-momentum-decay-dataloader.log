Starting epoch 1... 4063 batches total
  Batch 100/4063, Loss: 4.6977
    Avg batch data load: 0.75, Avg batch training: 0.30
  Batch 200/4063, Loss: 4.6308
    Avg batch data load: 0.60, Avg batch training: 0.29
  Batch 300/4063, Loss: 4.6150
    Avg batch data load: 0.46, Avg batch training: 0.30
  Batch 400/4063, Loss: 4.6060
    Avg batch data load: 0.40, Avg batch training: 0.29
  Batch 500/4063, Loss: 4.6832
    Avg batch data load: 0.36, Avg batch training: 0.29
  Batch 600/4063, Loss: 4.5866
    Avg batch data load: 0.33, Avg batch training: 0.29
  Batch 700/4063, Loss: 4.5652
    Avg batch data load: 0.31, Avg batch training: 0.29
  Batch 800/4063, Loss: 4.6062
    Avg batch data load: 0.30, Avg batch training: 0.29
  Batch 900/4063, Loss: 4.5532
    Avg batch data load: 0.29, Avg batch training: 0.30
  Batch 1000/4063, Loss: 4.6081
    Avg batch data load: 0.28, Avg batch training: 0.30
  Batch 1100/4063, Loss: 4.7132
    Avg batch data load: 0.28, Avg batch training: 0.30
  Batch 1200/4063, Loss: 4.5418
    Avg batch data load: 0.28, Avg batch training: 0.29
  Batch 1300/4063, Loss: 4.5346
    Avg batch data load: 0.28, Avg batch training: 0.29
  Batch 1400/4063, Loss: 4.4814
    Avg batch data load: 0.28, Avg batch training: 0.29
  Batch 1500/4063, Loss: 4.6583
    Avg batch data load: 0.28, Avg batch training: 0.29
  Batch 1600/4063, Loss: 4.4256
    Avg batch data load: 0.27, Avg batch training: 0.29
  Batch 1700/4063, Loss: 4.4511
    Avg batch data load: 0.27, Avg batch training: 0.29
  Batch 1800/4063, Loss: 4.4176
    Avg batch data load: 0.28, Avg batch training: 0.28
  Batch 1900/4063, Loss: 4.5115
    Avg batch data load: 0.27, Avg batch training: 0.29
  Batch 2000/4063, Loss: 4.3543
    Avg batch data load: 0.27, Avg batch training: 0.29
  Batch 2100/4063, Loss: 4.3336
    Avg batch data load: 0.27, Avg batch training: 0.29
  Batch 2200/4063, Loss: 4.4344
    Avg batch data load: 0.27, Avg batch training: 0.28
  Batch 2300/4063, Loss: 4.4689
    Avg batch data load: 0.27, Avg batch training: 0.28
  Batch 2400/4063, Loss: 4.2962
    Avg batch data load: 0.27, Avg batch training: 0.28
  Batch 2500/4063, Loss: 4.3404
    Avg batch data load: 0.27, Avg batch training: 0.28
  Batch 2600/4063, Loss: 4.4863
    Avg batch data load: 0.27, Avg batch training: 0.28
  Batch 2700/4063, Loss: 4.3349
    Avg batch data load: 0.27, Avg batch training: 0.28
  Batch 2800/4063, Loss: 4.5408
    Avg batch data load: 0.26, Avg batch training: 0.28
  Batch 2900/4063, Loss: 4.4763
    Avg batch data load: 0.26, Avg batch training: 0.28
  Batch 3000/4063, Loss: 4.4408
    Avg batch data load: 0.26, Avg batch training: 0.28
  Batch 3100/4063, Loss: 4.4284
    Avg batch data load: 0.26, Avg batch training: 0.28
  Batch 3200/4063, Loss: 4.2796
    Avg batch data load: 0.26, Avg batch training: 0.28
  Batch 3300/4063, Loss: 4.3800
    Avg batch data load: 0.26, Avg batch training: 0.28
  Batch 3400/4063, Loss: 4.4049
    Avg batch data load: 0.26, Avg batch training: 0.28
  Batch 3500/4063, Loss: 4.1782
    Avg batch data load: 0.26, Avg batch training: 0.28
  Batch 3600/4063, Loss: 4.4057
    Avg batch data load: 0.26, Avg batch training: 0.28
  Batch 3700/4063, Loss: 4.4753
    Avg batch data load: 0.26, Avg batch training: 0.28
  Batch 3800/4063, Loss: 4.2048
    Avg batch data load: 0.26, Avg batch training: 0.28
  Batch 3900/4063, Loss: 4.0743
    Avg batch data load: 0.26, Avg batch training: 0.28
  Batch 4000/4063, Loss: 4.2980
    Avg batch data load: 0.26, Avg batch training: 0.28
Finished epoch 1/1, Loss: 4.8968
  Avg epoch data load: 0.26, Avg epoch training: 0.28
  Total epoch data load: 1051.83, Total epoch training: 1139.75

Finished Training, Loss: 4.8968

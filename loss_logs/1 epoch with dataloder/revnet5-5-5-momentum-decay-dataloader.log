Starting epoch 1... 4063 batches total
  Batch 100/4063, Loss: 4.7397
    Avg batch data load: 0.12, Avg batch training: 1.77
  Batch 200/4063, Loss: 4.7044
    Avg batch data load: 0.06, Avg batch training: 1.75
  Batch 300/4063, Loss: 4.6128
    Avg batch data load: 0.04, Avg batch training: 1.75
  Batch 400/4063, Loss: 4.6351
    Avg batch data load: 0.03, Avg batch training: 1.75
  Batch 500/4063, Loss: 4.6648
    Avg batch data load: 0.03, Avg batch training: 1.74
  Batch 600/4063, Loss: 4.7244
    Avg batch data load: 0.02, Avg batch training: 1.75
  Batch 700/4063, Loss: 4.7152
    Avg batch data load: 0.02, Avg batch training: 1.75
  Batch 800/4063, Loss: 4.7271
    Avg batch data load: 0.02, Avg batch training: 1.76
  Batch 900/4063, Loss: 4.6151
    Avg batch data load: 0.02, Avg batch training: 1.76
  Batch 1000/4063, Loss: 4.5664
    Avg batch data load: 0.01, Avg batch training: 1.77
  Batch 1100/4063, Loss: 4.5049
    Avg batch data load: 0.01, Avg batch training: 1.77
  Batch 1200/4063, Loss: 4.5627
    Avg batch data load: 0.01, Avg batch training: 1.77
  Batch 1300/4063, Loss: 4.8483
    Avg batch data load: 0.01, Avg batch training: 1.78
  Batch 1400/4063, Loss: 4.5615
    Avg batch data load: 0.01, Avg batch training: 1.78
  Batch 1500/4063, Loss: 4.4132
    Avg batch data load: 0.01, Avg batch training: 1.78
  Batch 1600/4063, Loss: 4.6057
    Avg batch data load: 0.01, Avg batch training: 1.78
  Batch 1700/4063, Loss: 4.7021
    Avg batch data load: 0.01, Avg batch training: 1.78
  Batch 1800/4063, Loss: 4.4603
    Avg batch data load: 0.01, Avg batch training: 1.78
  Batch 1900/4063, Loss: 4.4308
    Avg batch data load: 0.01, Avg batch training: 1.78
  Batch 2000/4063, Loss: 4.3588
    Avg batch data load: 0.01, Avg batch training: 1.78
  Batch 2100/4063, Loss: 4.3688
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 2200/4063, Loss: 4.4093
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 2300/4063, Loss: 4.4735
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 2400/4063, Loss: 4.5156
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 2500/4063, Loss: 4.3786
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 2600/4063, Loss: 4.5398
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 2700/4063, Loss: 4.4461
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 2800/4063, Loss: 4.5475
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 2900/4063, Loss: 4.5487
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 3000/4063, Loss: 4.5177
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 3100/4063, Loss: 4.5756
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 3200/4063, Loss: 4.4616
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 3300/4063, Loss: 4.6145
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 3400/4063, Loss: 4.5423
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 3500/4063, Loss: 4.4149
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 3600/4063, Loss: 4.2794
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 3700/4063, Loss: 4.4067
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 3800/4063, Loss: 4.3518
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 3900/4063, Loss: 4.2547
    Avg batch data load: 0.01, Avg batch training: 1.79
  Batch 4000/4063, Loss: 4.7126
    Avg batch data load: 0.01, Avg batch training: 1.79
Finished epoch 1/1, Loss: 4.2795
  Avg epoch data load: 0.01, Avg epoch training: 1.79
  Total epoch data load: 22.00, Total epoch training: 7285.38

Finished Training, Loss: 4.2795

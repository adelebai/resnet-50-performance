Starting epoch 1... 4063 batches total
  Batch 100/4063, Loss: 4.9398
    Avg batch data load: 0.64, Avg batch training: 0.09
  Batch 200/4063, Loss: 4.7947
    Avg batch data load: 0.61, Avg batch training: 0.08
  Batch 300/4063, Loss: 4.6332
    Avg batch data load: 0.57, Avg batch training: 0.08
  Batch 400/4063, Loss: 4.7695
    Avg batch data load: 0.55, Avg batch training: 0.08
  Batch 500/4063, Loss: 4.7767
    Avg batch data load: 0.54, Avg batch training: 0.08
  Batch 600/4063, Loss: 4.8672
    Avg batch data load: 0.53, Avg batch training: 0.08
  Batch 700/4063, Loss: 4.6638
    Avg batch data load: 0.53, Avg batch training: 0.08
  Batch 800/4063, Loss: 4.6820
    Avg batch data load: 0.52, Avg batch training: 0.08
  Batch 900/4063, Loss: 4.4626
    Avg batch data load: 0.51, Avg batch training: 0.08
  Batch 1000/4063, Loss: 4.6319
    Avg batch data load: 0.51, Avg batch training: 0.08
  Batch 1100/4063, Loss: 4.5348
    Avg batch data load: 0.50, Avg batch training: 0.08
  Batch 1200/4063, Loss: 4.6627
    Avg batch data load: 0.50, Avg batch training: 0.08
  Batch 1300/4063, Loss: 4.5528
    Avg batch data load: 0.51, Avg batch training: 0.08
  Batch 1400/4063, Loss: 4.5106
    Avg batch data load: 0.51, Avg batch training: 0.08
  Batch 1500/4063, Loss: 4.6387
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 1600/4063, Loss: 4.2847
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 1700/4063, Loss: 4.5801
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 1800/4063, Loss: 4.2533
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 1900/4063, Loss: 4.1023
    Avg batch data load: 0.52, Avg batch training: 0.07
  Batch 2000/4063, Loss: 4.5174
    Avg batch data load: 0.52, Avg batch training: 0.07
  Batch 2100/4063, Loss: 4.2659
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 2200/4063, Loss: 4.5786
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 2300/4063, Loss: 4.2270
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 2400/4063, Loss: 4.5312
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 2500/4063, Loss: 4.5868
    Avg batch data load: 0.52, Avg batch training: 0.07
  Batch 2600/4063, Loss: 4.2993
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 2700/4063, Loss: 4.1273
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 2800/4063, Loss: 4.3083
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 2900/4063, Loss: 4.4197
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 3000/4063, Loss: 3.9883
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 3100/4063, Loss: 4.3239
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 3200/4063, Loss: 4.0695
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 3300/4063, Loss: 4.3004
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 3400/4063, Loss: 4.0308
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 3500/4063, Loss: 4.0311
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 3600/4063, Loss: 4.3070
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 3700/4063, Loss: 3.6773
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 3800/4063, Loss: 4.1332
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 3900/4063, Loss: 3.8334
    Avg batch data load: 0.51, Avg batch training: 0.07
  Batch 4000/4063, Loss: 3.6663
    Avg batch data load: 0.51, Avg batch training: 0.07
Finished epoch 1/1, Loss: 4.5197
  Avg epoch data load: 0.51, Avg epoch training: 0.07
  Total epoch data load: 2053.86, Total epoch training: 287.31

Finished Training, Loss: 4.5197
